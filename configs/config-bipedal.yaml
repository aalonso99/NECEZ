# Basic params
env_name: "BipedalWalker-v3"
obs_type: "bipedalwalker"
seed: 0
obs_size: [24]
action_dim: 4
action_size: 3
dim_action_values: [-1, 0, 1]

# Setup params  
log_dir: "runs"
log_name: "None"
load_buffer: False
debug: True
train_speed_profiling: False
get_batch_profiling: False
render: False
print_simple: True
max_games: 500 # Total number of games before training is ended
max_total_frames: 120000
max_frames: 1600 # Maximum frames for a single game before it is cut short
n_simulations: 30
try_cuda: True # Whether to use cuda if available (makes training slower on cartpole)


# Model params
latent_size: 6
support_width: 25

# Training params
initial_learning_rate: 0.2
final_learning_rate: 0.02
tr_steps_before_lr_decay: 100_000
weight_decay: 0.0001
grad_clip: 1 # 0 interpreted as no clipping
val_weight: 0.25
policy_weight: 1.0
batch_size: 256

# Search params
root_dirichlet_alpha: 0.3
explore_frac: 0.25
discount: 0.997
n_batches: 100
rollout_depth: 5
reward_depth: 30
buffer_size: 200

# Priority replay params
priority_replay: True
priority_alpha: 0.6
initial_priority_beta: 0.4
final_priority_beta: 1.0

# Temperature schedule
temp1: 10000 # Steps after which temperature is dropped to 0.5
temp2: 20000 # Steps after which temperature is dropped to 0.25
temp3: 100000 # Steps after which temperature is dropped to 0

# Reanalyse
reanalyse: True
reanalyse_n: 1
prior_weight: 1
momentum: 0.9

## EfficientZero Additions
# Value prefix
value_prefix: True
lstm_hidden_size: 64
val_prefix_size: 32
reward_channels: 16

# Off policy correction
off_policy_correction: True
tau: 0.3
reward_steps: 1000
total_training_steps: 120_000

# Consistency loss
consistency_loss: True
consistency_weight: 2.0
